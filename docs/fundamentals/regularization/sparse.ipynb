{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c00010",
   "metadata": {},
   "source": [
    "(lp-norm)=\n",
    "\n",
    "# Sparse regularization\n",
    "\n",
    "\n",
    "It is possible to generalize the conventional least-squares approach such that we can recover different solutions with variable degrees of sparsity. By sparsity, we mean fewer model cells (or gradients) away from the reference, but with larger values. The goal is to explore the model space by changing our assumption about the character of the solution, in terms of the volume of the anomalies and the sharpness of their edges. We can do this by changing the \"ruler\" by which we evaluate the model function $f(\\mathbf{m})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89bf21c",
   "metadata": {},
   "source": [
    "(reference-model)=\n",
    "\n",
    "## Smallness norm model\n",
    "\n",
    "The first option is to impose sparsity assumptions on the model values.\n",
    "\n",
    "$$\n",
    "\\phi(m) = \\| (\\mathbf{m} - \\mathbf{m}_{ref} \\|_p \\;.\n",
    "$$\n",
    "\n",
    "\n",
    "That is, we ask the inversion to recover anomalies that have small volumes but large physical property contrasts. This is desirable if we know the geological bodies to be discrete and compact (e.g. kimberlite pipe, dyke, etc.). The example below demonstrates the outcome of an inversion for different combinations of norms applied to the model\n",
    "\n",
    "```{figure} ./images/sparse_models.png\n",
    "---\n",
    "name: sparse_model\n",
    "---\n",
    "```\n",
    "\n",
    "The [figure above](sparse_model) shows vertical sections through the true and recovered models (from left to right) with L2, L1 and L0-norm on the model values.\n",
    "\n",
    "Note that as $p \\rightarrow 0$ the volume of the anomaly shrinks to a few non-zero elements while the physical property contrasts increase. They generally agree on the center position of the anomaly but differ significantly in their extent and shape.\n",
    "\n",
    "![smallness_setup](./images/smallness_norm_setup.png)\n",
    "\n",
    "No smoothness constraints were used ($\\alpha_{x,y,z} = 0$), only a penalty on the reference model ($m_{ref} = 0$).\n",
    "\n",
    "\n",
    "All those models fit the data to the target [data misfit](data-misfit) and are therefore valid solutions to the inversion.\n",
    "\n",
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24179e0b",
   "metadata": {},
   "source": [
    "(model-smoothness)=\n",
    "\n",
    "## Smoothness norm model\n",
    "\n",
    "Next, we explore the effect of applying sparse norms on the model gradients. We have two options.\n",
    "\n",
    "### Gradient type: Total\n",
    "\n",
    "The favoured approach is to measure the total gradient of the model\n",
    "\n",
    "$$\n",
    "\\phi(m) = \\| |\\nabla \\mathbf{m}| \\|_p\n",
    "$$\n",
    "\n",
    "or in matrix form\n",
    "\n",
    "$$\n",
    "\\phi(m) = \\| \\sqrt{(\\mathbf{G}_x \\mathbf{m})^2 + (\\mathbf{G}_y \\mathbf{m})^2 + (\\mathbf{G}_z \\mathbf{m})^2} \\|_p \\;.\n",
    "$$\n",
    "\n",
    "In this case, we are requesting a model that has either large or no (zero) gradients. This promotes anomalies with sharp contrast and constant values within.\n",
    "\n",
    "```{figure} ./images/flat_models.png\n",
    "---\n",
    "name: flat_model\n",
    "---\n",
    "```\n",
    "\n",
    "The [figure above](flat_model) shows vertical sections through the true and recovered models (from left to right) with L2, L1 and L0-norm on the model gradients.\n",
    "\n",
    "![smoothness_setup](./images/smoothness_norm_setup.png)\n",
    "\n",
    "No reference model was used ($\\alpha_s = 0$) with uniform norm values on all three Cartesian components.\n",
    "\n",
    "All those models also fit the data to the target [data misfit](data-misfit) and are therefore valid solutions to the inversion. Note that as $p \\rightarrow 0$ the edges of the anomaly become tighter while variability inside the body diminishes. They generally agree on the center position of the anomaly but differ greatly on the extent and shape.\n",
    "\n",
    "\n",
    "\n",
    "**Note**\n",
    "\n",
    "In the literature, the majority of studies on sparse norms use the L1-norm on the model gradients, also called `Total Variation` inversion. While the implementation may differ slightly, the general $l_p$-norm methodology presented above covers this specific case. We can re-write\n",
    "\n",
    "$$\n",
    "\\sum_{i} {|f(m)_i |}^p \\approx \\sum_{i} {\\frac{ (\\mathbf{G}_x \\mathbf{m})_i^2 + (\\mathbf{G}_y \\mathbf{m})_i^2 + (\\mathbf{G}_z \\mathbf{m})_i^2}{\\left(  (\\mathbf{G}_x \\mathbf{m})_i^2 + (\\mathbf{G}_y \\mathbf{m})_i^2 + (\\mathbf{G}_z \\mathbf{m})_i^2 + \\epsilon^2 \\right)^{1-p/2 }}} \\;.\n",
    "$$\n",
    "\n",
    "Then for $p=1$\n",
    "\n",
    "$$\n",
    "\\phi(m) \\approx \\sum_{i} \\sqrt{(\\mathbf{G}_x \\mathbf{m})_i^2 + (\\mathbf{G}_y \\mathbf{m})_i^2 + (\\mathbf{G}_z \\mathbf{m})_i^2} \\;,\n",
    "$$\n",
    "\n",
    "we recover the total-variation (TV) function. \n",
    "Lastly, for $p=2$\n",
    "\n",
    "$$\n",
    "\\phi(m) = \\sum_{i} (\\mathbf{G}_x \\mathbf{m})_i^2 + (\\mathbf{G}_y \\mathbf{m})_i^2 + (\\mathbf{G}_z \\mathbf{m})_i^2 \\;,\n",
    "$$\n",
    "\n",
    "we recover the smooth regularization.\n",
    "\n",
    "\n",
    "#### Gradient type: Components\n",
    "\n",
    "Alternatively, we can treat each gradient term independently, such that\n",
    "\n",
    "$$\n",
    "\\phi(m) = \\| \\mathbf{G}_x \\mathbf{m} \\|_p + \\| \\mathbf{G}_y \\mathbf{m} \\|_p + \\| \\mathbf{G}_z \\mathbf{m} \\|_p \\;.\n",
    "$$\n",
    "\n",
    "This formulation tends to recover box-like anomalies with edges aligned with the Cartesian axes. This is generally not desirable unless the gradients can be rotated in 3D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487f435",
   "metadata": {},
   "source": [
    "## Mixed norms\n",
    "\n",
    "The next logical step is to mix different norms on both the smallness and smoothness constraints. This gives rise to a \"space\" of models bounded by all possible combinations on $[0, 2]$ applied to each function independently.\n",
    "\n",
    "To be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6527c-842e-4b8f-8619-bb96ff3deffc",
   "metadata": {},
   "source": [
    "### Background: Approximated $L_p$-norm\n",
    "\n",
    "The standard $L_p$ norm measure is generally written as\n",
    "\n",
    "$$\n",
    "\\phi(m) = \\| f(\\mathbf{m}) \\|_p ] \\;,\n",
    "$$\n",
    "\n",
    "which can also be expressed as\n",
    "\n",
    "$$\n",
    "\\phi(m) = \\sum_{j}^M {|f(m)_i |}^p \\;.\n",
    "$$\n",
    "\n",
    "For $p<=1$, the function is non-linear with respect to the model. It is possible to approximate the function in a linearized form as  \n",
    "\n",
    "$$\n",
    "\\sum_{j}^M {|f(m)_i |}^p \\approx \\sum_{i} {\\frac{ {f(m)_i}^2}{\\left( {{f(m)_i}}^{2} + \\epsilon^2 \\right)^{1-p/2 }}}\n",
    "$$\n",
    "\n",
    "where $p$ is any constant between $[0,\\;2]$. This is a decent approximation of the $l_p$-norm for any of the functions presented in the [L2-norm](l2-norm) section.\n",
    "\n",
    "![lp_norm](./images/lp_norm.png)\n",
    "\n",
    "Note that choosing a different $l_p$-norm greatly changes how we measure the function $f(m)$. Rather than simply increasing exponentially with the size of $f(m)$, small norms increase the penalty on low $f(m)$ values.\n",
    "As $p\\rightarrow 0$, we attempt to recover as few non-zero elements, which in turn favour sparse solutions.\n",
    "\n",
    "Since it is a non-linear function with respect to $\\mathbf{m}$, we must first linearize it as\n",
    "\n",
    "$$\n",
    "\\| f(\\mathbf{m})_i \\|_p \\approx \\| \\mathbf{R}_i \\mathbf{W}_i f(\\mathbf{m})_i\\|\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_i = {\\frac{1}{\\left( {{f(m)^{(k-1)}_i}}^{2} + \\epsilon^2 \\right)^{1-p/2 }}}\\,.\n",
    "$$\n",
    "\n",
    "Here, the superscript $(k)$ denotes the iteration step of the inversion. This is also known as an iterative re-weighted least-squares (IRLS) method. For more details on the implementation, refer to {cite:t}`fournier_2019`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf887ab",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
